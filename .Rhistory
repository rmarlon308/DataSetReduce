#Fit a model of sparrow survival probability
#Se prueba la regresion logistica
#Esta regresion esta entre 0 y 1, utiliza el pseudo R2 para ver si se tiene un buen modelo
sparrow = readRDS("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/sparrow.rds")
library(broom)
# sparrow is in the workspace
summary(sparrow)
# Create the survived column
sparrow$survived <- sparrow$status == "Survived" #Se reemplaza por true o false
# Create the formula
(fmla <- survived ~ total_length + weight + humerus)
# Fit the logistic regression model
sparrow_model <- glm(fmla, data = sparrow, family = binomial)
# Call summary
summary(sparrow_model)
# Call glance
(perf <- glance(sparrow_model)) #Permite sacar facilamente los pseudo R2
# Calculate pseudo-R-squared
(pseudoR2 <- 1 - (perf$deviance/ perf$null.deviance)) # No es un buen modelo porque no se acerca a 1
#Se hacen predicciones con el modelo de regresion logistica
#Se prueba la curva de ganancia
#Entre mas cerca este de la curva de ganancia es mejor
#GainCurvePlot(frame, xvar, truthVar, title)
# frame: data frame with prediction column and ground truth column
# xvar: the name of the column of predictions (as a string)
# truthVar: the name of the column with actual outcome (as a string)
# title: a title for the plot (as a string)
# sparrow is in the workspace
summary(sparrow)
# sparrow_model is in the workspace
summary(sparrow_model)
# Make predictions
sparrow$pred <- predict(sparrow_model, type = "response")
library(WVPlots)
# Look at gain curve
GainCurvePlot(sparrow, "pred", "survived", "sparrow survival model")
# You see from the gain curve that the model follows the wizard curve for about the first 30% of the data, identifying about 45% of the surviving sparrows with only a few false positives.
#Poisson and Quasipoisson
#Se usa para predecir datos de conteo
#Fit a model to predict bike rental counts
library(miceadds)
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Bikes.RData", "bikesJuly")
#Para que hacemos estas variables???????
outcome = "cnt"
vars = c("hr","holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed")
#Las usamos para simplificar la formula que se va a utilizar
# bikesJuly is in the workspace
str(bikesJuly)
# The outcome column
outcome
# The inputs to use
vars
# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + "))) #Esta formula esta hecha con las varibles de arriba
# Calculate the mean and variance of the outcome
(mean_bikes <- mean(bikesJuly$cnt))
(var_bikes <- var(bikesJuly$cnt))
#Si la media y la varianza son mas o menos similares se usa como familia poisson, si no son parecidas se usa quasipoisson
#En este caso no son iguales
# Fit the model
# Notese que se usa como familia quasipoisson
bike_model <- glm(fmla, data = bikesJuly, family = quasipoisson)
# Call glance
#Usamos glance para obtener la desviacion y la desviacion nula para poder hacer el pseudo R2 y ver si nuestro modelo es bueno
(perf <- glance(bike_model))
# Calculate pseudo-R-squared
#El modelo es mas o menos bueno con 0.8 de pseudo R2.
#Pseudo R2 tiene que estar cerca de 1
(pseudoR2 <- 1 - (perf$deviance/perf$null.deviance))
#Predict bike rentals on new data
#Se realizan predicciones con la regresion logistica
View(bikesJuly)
#Fit a model of sparrow survival probability
#Se prueba la regresion logistica
#Esta regresion esta entre 0 y 1, utiliza el pseudo R2 para ver si se tiene un buen modelo
sparrow = readRDS("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/sparrow.rds")
library(broom)
# sparrow is in the workspace
summary(sparrow)
# Create the survived column
sparrow$survived <- sparrow$status == "Survived" #Se reemplaza por true o false
# Create the formula
(fmla <- survived ~ total_length + weight + humerus)
# Fit the logistic regression model
sparrow_model <- glm(fmla, data = sparrow, family = binomial)
# Call summary
summary(sparrow_model)
# Call glance
(perf <- glance(sparrow_model)) #Permite sacar facilamente los pseudo R2
# Calculate pseudo-R-squared
(pseudoR2 <- 1 - (perf$deviance/ perf$null.deviance)) # No es un buen modelo porque no se acerca a 1
#Se hacen predicciones con el modelo de regresion logistica
#Se prueba la curva de ganancia
#Entre mas cerca este de la curva de ganancia es mejor
#GainCurvePlot(frame, xvar, truthVar, title)
# frame: data frame with prediction column and ground truth column
# xvar: the name of the column of predictions (as a string)
# truthVar: the name of the column with actual outcome (as a string)
# title: a title for the plot (as a string)
# sparrow is in the workspace
summary(sparrow)
# sparrow_model is in the workspace
summary(sparrow_model)
# Make predictions
sparrow$pred <- predict(sparrow_model, type = "response")
library(WVPlots)
# Look at gain curve
GainCurvePlot(sparrow, "pred", "survived", "sparrow survival model")
# You see from the gain curve that the model follows the wizard curve for about the first 30% of the data, identifying about 45% of the surviving sparrows with only a few false positives.
#Poisson and Quasipoisson
#Se usa para predecir datos de conteo
#Fit a model to predict bike rental counts
library(miceadds)
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Bikes.RData", "bikesJuly")
#Para que hacemos estas variables???????
outcome = "cnt"
vars = c("hr","holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed")
#Las usamos para simplificar la formula que se va a utilizar
bikesAugust = bikesJuly
# bikesJuly is in the workspace
str(bikesJuly)
# The outcome column
outcome
# The inputs to use
vars
# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + "))) #Esta formula esta hecha con las varibles de arriba
# Calculate the mean and variance of the outcome
(mean_bikes <- mean(bikesJuly$cnt))
(var_bikes <- var(bikesJuly$cnt))
#Si la media y la varianza son mas o menos similares se usa como familia poisson, si no son parecidas se usa quasipoisson
#En este caso no son iguales
# Fit the model
# Notese que se usa como familia quasipoisson
bike_model <- glm(fmla, data = bikesJuly, family = quasipoisson)
# Call glance
#Usamos glance para obtener la desviacion y la desviacion nula para poder hacer el pseudo R2 y ver si nuestro modelo es bueno
(perf <- glance(bike_model))
# Calculate pseudo-R-squared
#El modelo es mas o menos bueno con 0.8 de pseudo R2.
#Pseudo R2 tiene que estar cerca de 1
(pseudoR2 <- 1 - (perf$deviance/perf$null.deviance))
#Predict bike rentals on new data
#Se realizan predicciones con la regresion logistica
# bikesAugust is in the workspace
str(bikesAugust) #BikesAugust es la newdata
# bike_model is in the workspace
summary(bike_model)
# Make predictions on August data
bikesAugust$pred  <- predict(bike_model, newdata = bikesAugust, type = "response")
# Calculate the RMSE
bikesAugust %>%
mutate(residual = cnt - pred) %>%
summarize(rmse  = sqrt(mean(residual^2)))
# Plot predictions vs cnt (pred on x-axis)
ggplot(bikesAugust, aes(x = pred, y = cnt)) +
geom_point() +
geom_abline(color = "darkblue")
library(ggplot2)
ggplot(bikesAugust, aes(x = pred, y = cnt)) +
geom_point() +
geom_abline(color = "darkblue")
library(tidyr)
library(tidyr)
# Plot predictions and cnt by date/time
bikesAugust %>%
# set start to 0, convert unit to days
mutate(instant = (instant - min(instant))/24) %>%  #Se convierte a dias
# gather cnt and pred into a value column
gather(key = valuetype, value = value, cnt, pred) %>%
filter(instant < 14) %>% # restric to first 14 days
# plot value by instant
ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) +
geom_point() +
geom_line() +
scale_x_continuous("Day", breaks = 0:14, labels = 0:14) +
scale_color_brewer(palette = "Dark2") +
ggtitle("Predicted August bike rentals, Quasipoisson model")
library(tidyr)
library(dplyr)
# Plot predictions and cnt by date/time
bikesAugust %>%
# set start to 0, convert unit to days
mutate(instant = (instant - min(instant))/24) %>%  #Se convierte a dias
# gather cnt and pred into a value column
gather(key = valuetype, value = value, cnt, pred) %>%
filter(instant < 14) %>% # restric to first 14 days
# plot value by instant
ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) +
geom_point() +
geom_line() +
scale_x_continuous("Day", breaks = 0:14, labels = 0:14) +
scale_color_brewer(palette = "Dark2") +
ggtitle("Predicted August bike rentals, Quasipoisson model")
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Soybean.RData", "soybean_train")
summary(soybean_train)
# soybean_train is in the workspace
summary(soybean_train)
# Plot weight vs Time (Time on x axis)
ggplot(soybean_train, aes(x = Time, y = weight)) +
geom_point()
# Load the package mgcv
library(mgcv)
# Create the formula
(fmla.gam <- weight ~ s(Time)) #Time no tiene una relacion lineal
# Fit the GAM Model
model.gam <- gam(fmla.gam, family = gaussian, data= soybean_train)
# Call summary() on model.lin and look for R-squared
summary(model.lin)
# Call summary() on model.gam and look for R-squared
summary(model.gam)
# Call plot() on model.gam
plot(model.gam)
# Call summary() on model.lin and look for R-squared
summary(model.lin)
#Fit a model of sparrow survival probability
#Se prueba la regresion logistica
#Esta regresion esta entre 0 y 1, utiliza el pseudo R2 para ver si se tiene un buen modelo
sparrow = readRDS("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/sparrow.rds")
library(broom)
# sparrow is in the workspace
summary(sparrow)
# Create the survived column
sparrow$survived <- sparrow$status == "Survived" #Se reemplaza por true o false
# Create the formula
(fmla <- survived ~ total_length + weight + humerus)
# Fit the logistic regression model
sparrow_model <- glm(fmla, data = sparrow, family = binomial)
# Call summary
summary(sparrow_model)
# Call glance
(perf <- glance(sparrow_model)) #Permite sacar facilamente los pseudo R2
# Calculate pseudo-R-squared
(pseudoR2 <- 1 - (perf$deviance/ perf$null.deviance)) # No es un buen modelo porque no se acerca a 1
#Se hacen predicciones con el modelo de regresion logistica
#Se prueba la curva de ganancia
#Entre mas cerca este de la curva de ganancia es mejor
#GainCurvePlot(frame, xvar, truthVar, title)
# frame: data frame with prediction column and ground truth column
# xvar: the name of the column of predictions (as a string)
# truthVar: the name of the column with actual outcome (as a string)
# title: a title for the plot (as a string)
# sparrow is in the workspace
summary(sparrow)
# sparrow_model is in the workspace
summary(sparrow_model)
# Make predictions
sparrow$pred <- predict(sparrow_model, type = "response")
library(WVPlots)
# Look at gain curve
GainCurvePlot(sparrow, "pred", "survived", "sparrow survival model")
# You see from the gain curve that the model follows the wizard curve for about the first 30% of the data, identifying about 45% of the surviving sparrows with only a few false positives.
#Poisson and Quasipoisson
#Se usa para predecir datos de conteo
#Fit a model to predict bike rental counts
library(miceadds)
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Bikes.RData", "bikesJuly")
#Para que hacemos estas variables???????
outcome = "cnt"
vars = c("hr","holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed")
#Las usamos para simplificar la formula que se va a utilizar
bikesAugust = bikesJuly
# bikesJuly is in the workspace
str(bikesJuly)
# The outcome column
outcome
# The inputs to use
vars
# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + "))) #Esta formula esta hecha con las varibles de arriba
# Calculate the mean and variance of the outcome
(mean_bikes <- mean(bikesJuly$cnt))
(var_bikes <- var(bikesJuly$cnt))
#Si la media y la varianza son mas o menos similares se usa como familia poisson, si no son parecidas se usa quasipoisson
#En este caso no son iguales
# Fit the model
# Notese que se usa como familia quasipoisson
bike_model <- glm(fmla, data = bikesJuly, family = quasipoisson)
# Call glance
#Usamos glance para obtener la desviacion y la desviacion nula para poder hacer el pseudo R2 y ver si nuestro modelo es bueno
(perf <- glance(bike_model))
# Calculate pseudo-R-squared
#El modelo es mas o menos bueno con 0.8 de pseudo R2.
#Pseudo R2 tiene que estar cerca de 1
(pseudoR2 <- 1 - (perf$deviance/perf$null.deviance))
#Predict bike rentals on new data
#Se realizan predicciones con la regresion logistica
# bikesAugust is in the workspace
str(bikesAugust) #BikesAugust es la newdata
# bike_model is in the workspace
summary(bike_model)
# Make predictions on August data
bikesAugust$pred  <- predict(bike_model, newdata = bikesAugust, type = "response")
# Calculate the RMSE
bikesAugust %>%
mutate(residual = cnt - pred) %>%
summarize(rmse  = sqrt(mean(residual^2)))
library(ggplot2)
# Plot predictions vs cnt (pred on x-axis)
ggplot(bikesAugust, aes(x = pred, y = cnt)) +
geom_point() +
geom_abline(color = "darkblue")
#Visualize the bike rental predictions
#Se ve como el modelo performa en el tiempo
library(tidyr)
library(dplyr)
# Plot predictions and cnt by date/time
bikesAugust %>%
# set start to 0, convert unit to days
mutate(instant = (instant - min(instant))/24) %>%  #Se convierte a dias
# gather cnt and pred into a value column
gather(key = valuetype, value = value, cnt, pred) %>%
filter(instant < 14) %>% # restric to first 14 days
# plot value by instant
ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) +
geom_point() +
geom_line() +
scale_x_continuous("Day", breaks = 0:14, labels = 0:14) +
scale_color_brewer(palette = "Dark2") +
ggtitle("Predicted August bike rentals, Quasipoisson model")
#GAMs
#Model soybean growth with GAM
library(miceadds)
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Soybean.RData", "soybean")
N = nrow(soybean)
target = round(N * 0.75)
gp = runif(N)
library(miceadds)
load.Rdata("/home/marlon/Documents/machine_learning_R/Supervised Learning in R: Regression/Soybean.RData", "soybean")
N = nrow(soybean)
target = round(N * 0.75)
gp = runif(N)
soybean_train = soybean[gp < 0.75, ]
#test data
soybean_test = soybean[gp >= 0.75, ]
# soybean_train is in the workspace
summary(soybean_train)
summary(soybean_train)
# Plot weight vs Time (Time on x axis)
ggplot(soybean_train, aes(x = Time, y = weight)) +
geom_point()
# Load the package mgcv
library(mgcv)
# Create the formula
(fmla.gam <- weight ~ s(Time)) #Time no tiene una relacion lineal
# Fit the GAM Model
model.gam <- gam(fmla.gam, family = gaussian, data= soybean_train)
# Call summary() on model.lin and look for R-squared
#summary(model.lin) Este el modelo lineal
# Call summary() on model.gam and look for R-squared
summary(model.gam)
# Call plot() on model.gam
plot(model.gam)
# soybean_test is in the workspace
summary(soybean_test)
#Predict with the soybean model on test data
# soybean_test is in the workspace
summary(soybean_test)
# soybean_test is in the workspace
summary(soybean_test)
# Get predictions from linear model
soybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)
# Get predictions from gam model
soybean_test$pred.gam <- as.numeric(predict(model.gam, newdata = soybean_test))
# Gather the predictions into a "long" dataset
soybean_long <- soybean_test %>%
gather(key = modeltype, value = pred, pred.lin, pred.gam)
# Calculate the rmse
soybean_long %>%
mutate(residual = weight - pred) %>%     # residuals
group_by(modeltype) %>%                  # group by modeltype
summarize(rmse = sqrt(mean(residual^2))) # calculate the RMSE
# Compare the predictions against actual weights on the test data
soybean_long %>%
ggplot(aes(x = Time)) +                          # the column for the x axis
geom_point(aes(y = weight)) +                    # the y-column for the scatterplot
geom_point(aes(y = pred, color = modeltype)) +   # the y-column for the point-and-line plot
geom_line(aes(y = pred, color = modeltype, linetype = modeltype)) + # the y-column for the point-and-line plot
scale_color_brewer(palette = "Dark2")
# Get predictions from linear model
soybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)
(fmla.lin <- weight ~ Time)
model.lin <- gam(fmla.lin, family = gaussian, data= soybean_train)
# Load the package mgcv
library(mgcv)
# Create the formula
(fmla.gam <- weight ~ s(Time)) #Time no tiene una relacion lineal
# Fit the GAM Model
model.gam <- gam(fmla.gam, family = gaussian, data= soybean_train)
(fmla.lin <- weight ~ Time)
model.lin <- gam(fmla.lin, family = gaussian, data= soybean_train)
# Call summary() on model.lin and look for R-squared
#summary(model.lin) Este el modelo lineal
# Call summary() on model.gam and look for R-squared
summary(model.gam)
# Call plot() on model.gam
plot(model.gam)
#Predict with the soybean model on test data
# soybean_test is in the workspace
summary(soybean_test)
# soybean_test is in the workspace
summary(soybean_test)
# Get predictions from linear model
soybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)
# Get predictions from gam model
soybean_test$pred.gam <- as.numeric(predict(model.gam, newdata = soybean_test))
# Gather the predictions into a "long" dataset
soybean_long <- soybean_test %>%
gather(key = modeltype, value = pred, pred.lin, pred.gam)
# Calculate the rmse
soybean_long %>%
mutate(residual = weight - pred) %>%     # residuals
group_by(modeltype) %>%                  # group by modeltype
summarize(rmse = sqrt(mean(residual^2))) # calculate the RMSE
# Compare the predictions against actual weights on the test data
soybean_long %>%
ggplot(aes(x = Time)) +                          # the column for the x axis
geom_point(aes(y = weight)) +                    # the y-column for the scatterplot
geom_point(aes(y = pred, color = modeltype)) +   # the y-column for the point-and-line plot
geom_line(aes(y = pred, color = modeltype, linetype = modeltype)) + # the y-column for the point-and-line plot
scale_color_brewer(palette = "Dark2")
head(livetweets_data)
library(readr)
livetweets_data <- read_delim("/home/marlon/mainfolder/marlon/USFQ/DataMining/3_TextMining/P3/livetweets_data.csv",
"|", escape_double = FALSE, trim_ws = TRUE)
head(livetweets_data)
library(dplyr)
library(dplyr)
livetweets_data = livetweets_data %>%
select(tweet_text, tweet_screen_name)
head(livetweets_data)
saveRDS(livetweets_data, file = "livetweets_data.rds")
saveRDS(livetweets_data, file = "/home/marlon/mainfolder/marlon/USFQ/DataMining/3_TextMining/P3/livetweets_data.rds")
library(readr)
library(dplyr)
library(FSelector)
setwd("/home/marlon/mainfolder/marlon/USFQ/DataMining/4_ProcesamientoDatos/P4")
SPECTF_train <- read_csv("SPECTF.train", col_names = FALSE)
SPECTF_test <- read_csv("SPECTF.test", col_names = FALSE)
SPECTF = rbind(SPECTF_train, SPECTF_test)
names(SPECTF) = c("OVERALL_DIAGNOSIS", "F1R", "F1S","F2R","F2S","F3R","F3S","F4R","F4S","F5R","F5S","F6R","F6S","F7R","F7S","F8R","F8S","F9R","F9S","F10R","F10S","F11R","F11S","F12R","F12S","F13R","F13S","F14R","F14S","F15R","F15S","F16R","F16S","F17R","F17S","F18R","F18S","F19R","F19S","F20R","F20S","F21R","F21S","F22R","F22S")
normalized_min_max = SPECTF
for(i in 2:ncol(normalized_min_max)){
normalized_min_max[i] =  (normalized_min_max[i] - min(normalized_min_max[i]))/ (max(normalized_min_max[i] - min(normalized_min_max[i]))) * (1-0) + 0
}
weights <- information.gain(OVERALL_DIAGNOSIS~., normalized_min_max)
important_attributes = weights %>%
filter(attr_importance != 0) %>%
arrange(desc(attr_importance))
SPECTF_filter = normalized_min_max[, which((names(normalized_min_max) %in% row.names(important_attributes)) == T)]
correlation_matrix = as.data.frame(cor(SPECTF_filter))
Final_data_SPECTF = normalized_min_max %>%
select(OVERALL_DIAGNOSIS, F3S, F13S, F20S, F21R)
saveRDS(Final_data_SPECTF, "SPECTF_FINAL.rds")
outlook = as.factor(c("Sunny", "Sunny", "Overcast", "Rainy", "Rainy", "Rainy", "Overcast","Sunny", "Sunny","Rainy","Sunny", "Overcast", "Overcast", "Rainy"))
temperature = as.factor(c("Hot","Hot","Hot","Mild", "Cool","Cool","Cool","Mild", "Cool","Mild","Mild","Mild","Hot", "Mild"))
Humidity = as.factor(c("High","High","High","High","Normal","Normal","Normal","High","Normal","Normal","Normal", "High","Normal", "High"))
windy = as.factor(c("False","True","False","False","False","True","True","False","False","False", "True","True","False","True"))
play = as.factor(c("No","No","Yes","Yes","Yes","No","Yes","No","Yes","Yes","Yes","Yes","Yes","No"))
test_data_set = data.frame(outlook, temperature, Humidity,windy,play)
weights_2 <- information.gain(play~., test_data_set, unit = "log2")
View(weights_2)
library(readr)
library(dplyr)
library(FSelector)
setwd("/home/marlon/mainfolder/marlon/USFQ/DataMining/4_ProcesamientoDatos/P4")
SPECTF_train <- read_csv("SPECTF.train", col_names = FALSE)
SPECTF_test <- read_csv("SPECTF.test", col_names = FALSE)
SPECTF = rbind(SPECTF_train, SPECTF_test)
names(SPECTF) = c("OVERALL_DIAGNOSIS", "F1R", "F1S","F2R","F2S","F3R","F3S","F4R","F4S","F5R","F5S","F6R","F6S","F7R","F7S","F8R","F8S","F9R","F9S","F10R","F10S","F11R","F11S","F12R","F12S","F13R","F13S","F14R","F14S","F15R","F15S","F16R","F16S","F17R","F17S","F18R","F18S","F19R","F19S","F20R","F20S","F21R","F21S","F22R","F22S")
normalized_min_max = SPECTF
for(i in 2:ncol(normalized_min_max)){
normalized_min_max[i] =  (normalized_min_max[i] - min(normalized_min_max[i]))/ (max(normalized_min_max[i] - min(normalized_min_max[i]))) * (1-0) + 0
}
weights <- information.gain(OVERALL_DIAGNOSIS~., normalized_min_max, unit = "log2")
View(weights)
library(readr)
library(dplyr)
library(FSelector)
setwd("/home/marlon/mainfolder/marlon/USFQ/DataMining/4_ProcesamientoDatos/P4")
SPECTF_train <- read_csv("SPECTF.train", col_names = FALSE)
SPECTF_test <- read_csv("SPECTF.test", col_names = FALSE)
SPECTF = rbind(SPECTF_train, SPECTF_test)
names(SPECTF) = c("OVERALL_DIAGNOSIS", "F1R", "F1S","F2R","F2S","F3R","F3S","F4R","F4S","F5R","F5S","F6R","F6S","F7R","F7S","F8R","F8S","F9R","F9S","F10R","F10S","F11R","F11S","F12R","F12S","F13R","F13S","F14R","F14S","F15R","F15S","F16R","F16S","F17R","F17S","F18R","F18S","F19R","F19S","F20R","F20S","F21R","F21S","F22R","F22S")
normalized_min_max = SPECTF
for(i in 2:ncol(normalized_min_max)){
normalized_min_max[i] =  (normalized_min_max[i] - min(normalized_min_max[i]))/ (max(normalized_min_max[i] - min(normalized_min_max[i]))) * (1-0) + 0
}
weights <- information.gain(OVERALL_DIAGNOSIS~., normalized_min_max, unit = "log2")
View(weights)
library(readr)
library(dplyr)
library(FSelector)
setwd("/home/marlon/mainfolder/marlon/USFQ/DataMining/4_ProcesamientoDatos/P4")
SPECTF_train <- read_csv("SPECTF.train", col_names = FALSE)
SPECTF_test <- read_csv("SPECTF.test", col_names = FALSE)
SPECTF = rbind(SPECTF_train, SPECTF_test)
names(SPECTF) = c("OVERALL_DIAGNOSIS", "F1R", "F1S","F2R","F2S","F3R","F3S","F4R","F4S","F5R","F5S","F6R","F6S","F7R","F7S","F8R","F8S","F9R","F9S","F10R","F10S","F11R","F11S","F12R","F12S","F13R","F13S","F14R","F14S","F15R","F15S","F16R","F16S","F17R","F17S","F18R","F18S","F19R","F19S","F20R","F20S","F21R","F21S","F22R","F22S")
normalized_min_max = SPECTF
for(i in 2:ncol(normalized_min_max)){
normalized_min_max[i] =  (normalized_min_max[i] - min(normalized_min_max[i]))/ (max(normalized_min_max[i] - min(normalized_min_max[i]))) * (1-0) + 0
}
weights <- information.gain(OVERALL_DIAGNOSIS~., normalized_min_max, unit = "log2")
important_attributes = weights %>%
filter(attr_importance != 0) %>%
arrange(desc(attr_importance))
SPECTF_filter = normalized_min_max[, which((names(normalized_min_max) %in% row.names(important_attributes)) == T)]
correlation_matrix = as.data.frame(cor(SPECTF_filter))
Final_data_SPECTF = normalized_min_max %>%
select(OVERALL_DIAGNOSIS, F3S, F13S, F20S, F21R)
saveRDS(Final_data_SPECTF, "SPECTF_FINAL.rds")
View(correlation_matrix)
